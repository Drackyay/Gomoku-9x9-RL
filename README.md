# ğŸ® Gomoku 9Ã—9 AI with AlphaZero

A reinforcement learning project that trains an AI to play Gomoku (Five in a Row) using the **AlphaZero algorithm** (MCTS + Neural Network).

## ğŸ“‹ Project Structure

```
gomoku-9x9-ppo/
â”œâ”€â”€ env/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ gomoku_env.py      # Gymnasium environment
â”‚   â””â”€â”€ test_env.py        # Environment tests
â”œâ”€â”€ rl/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ alphazero_train.py # AlphaZero training (MCTS + NN)
â”‚   â””â”€â”€ mcts_ai.py         # Monte Carlo Tree Search
â”œâ”€â”€ gui/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ play_gui.py        # Streamlit GUI
â”œâ”€â”€ models/                 # Saved models
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
python -m venv venv
.\venv\Scripts\activate  # Windows
pip install -r requirements.txt
```

### 2. Play Against AI (MCTS)

```bash
streamlit run gui/play_gui.py
```

Open http://localhost:8501 in your browser.

### 3. Train AlphaZero Model

```bash
python rl/alphazero_train.py --iterations 30 --games 50 --simulations 100
```

## ğŸ§  How It Works

### AlphaZero Algorithm

This project implements the **AlphaZero** approach:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ALPHAZERO TRAINING            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  1. SELF-PLAY                          â”‚
â”‚     MCTS (guided by NN) plays games    â”‚
â”‚     against itself                      â”‚
â”‚                                         â”‚
â”‚  2. TRAINING                           â”‚
â”‚     Neural network learns to predict:   â”‚
â”‚     - Move probabilities (policy)       â”‚
â”‚     - Position value (who's winning)    â”‚
â”‚                                         â”‚
â”‚  3. REPEAT                             â”‚
â”‚     Better NN â†’ Better MCTS â†’ Better   â”‚
â”‚     training data â†’ Even better NN     â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

| Component | Description |
|-----------|-------------|
| **Neural Network** | ResNet that evaluates positions and suggests moves |
| **MCTS** | Monte Carlo Tree Search for move selection |
| **Self-Play** | Generate training data by playing against itself |

### Difficulty Levels

| Level | MCTS Simulations | Thinking Time |
|-------|------------------|---------------|
| Easy | 50 | ~0.5s |
| Medium | 200 | ~2s |
| Hard | 500 | ~4s |

## ğŸ¯ Game Rules

- **Board**: 9Ã—9 grid
- **Players**: Black (âš«) vs White (âšª)
- **Objective**: Get 5 stones in a row (horizontal, vertical, or diagonal)
- **Black moves first**

## ğŸ“Š Training Details

### AlphaZero Configuration

```python
GomokuNet:
  - 5 Residual Blocks
  - 128 channels
  - Policy head: predicts move probabilities
  - Value head: predicts game outcome

MCTS:
  - 100 simulations per move
  - UCB exploration constant: 1.5
  - Temperature: 1.0 (early game), 0.1 (late game)
```

### Training Loop

1. **Self-play**: 50 games per iteration
2. **Training**: Update neural network
3. **Evaluation**: Test against heuristic opponent
4. **Repeat**: 30 iterations total

## ğŸ”§ Requirements

- Python 3.8+
- PyTorch 2.0+ (CUDA recommended)
- Streamlit
- NumPy

## ğŸ“ School Project

**TAC450** - Reinforcement Learning Project

This project demonstrates the AlphaZero algorithm applied to Gomoku, showcasing how combining neural networks with Monte Carlo Tree Search creates a powerful game-playing AI.

## ğŸ“š References

- [AlphaGo Zero Paper](https://www.nature.com/articles/nature24270)
- [AlphaZero Paper](https://arxiv.org/abs/1712.01815)

## License

MIT License - For educational purposes
